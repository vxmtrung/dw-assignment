{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27a3e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:24.619191Z",
     "iopub.status.busy": "2025-04-05T16:54:24.618822Z",
     "iopub.status.idle": "2025-04-05T16:54:28.863971Z",
     "shell.execute_reply": "2025-04-05T16:54:28.862550Z"
    },
    "papermill": {
     "duration": 4.252339,
     "end_time": "2025-04-05T16:54:28.866061",
     "exception": false,
     "start_time": "2025-04-05T16:54:24.613722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62f5d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:28.876997Z",
     "iopub.status.busy": "2025-04-05T16:54:28.876689Z",
     "iopub.status.idle": "2025-04-05T16:54:35.055911Z",
     "shell.execute_reply": "2025-04-05T16:54:35.055201Z"
    },
    "papermill": {
     "duration": 6.185183,
     "end_time": "2025-04-05T16:54:35.057426",
     "exception": false,
     "start_time": "2025-04-05T16:54:28.872243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from datetime import datetime  \n",
    "import joblib  \n",
    "import os  \n",
    "import pickle  \n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder  \n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve  \n",
    "from sklearn.impute import SimpleImputer  \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset  \n",
    "from torch.nn import functional as F  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd31594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.066108Z",
     "iopub.status.busy": "2025-04-05T16:54:35.065694Z",
     "iopub.status.idle": "2025-04-05T16:54:35.072479Z",
     "shell.execute_reply": "2025-04-05T16:54:35.071856Z"
    },
    "papermill": {
     "duration": 0.012387,
     "end_time": "2025-04-05T16:54:35.073752",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.061365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):  \n",
    "    \"\"\"  \n",
    "    Preprocess the accident dataset for model training  \n",
    "    \n",
    "    Args:  \n",
    "        df: Pandas DataFrame with accident data  \n",
    "        \n",
    "    Returns:  \n",
    "        Processed features and target  \n",
    "    \"\"\"  \n",
    "    print(\"Starting data preprocessing...\")  \n",
    "    \n",
    "    df = df.dropna()\n",
    "    def process_datetime(date_str):  \n",
    "        try:  \n",
    "            # Try different date formats  \n",
    "            for fmt in ['%m/%d/%Y %I:%M:%S %p', '%m/%d/%Y %H:%M:%S']:  \n",
    "                try:  \n",
    "                    date_obj = datetime.strptime(date_str, fmt)  \n",
    "                    return date_obj  \n",
    "                except ValueError:  \n",
    "                    continue  \n",
    "            return pd.NaT  \n",
    "        except:  \n",
    "            return pd.NaT  \n",
    "\n",
    "    \n",
    "    if 'crash_type' in df.columns:  \n",
    "        df['target'] = (df['crash_type'] == 'INJURY AND / OR TOW DUE TO CRASH').astype(int)  \n",
    "        print(f\"Target distribution: {df['target'].value_counts().to_dict()}\")  \n",
    "    \n",
    "    categorical_features = [  \n",
    "        'traffic_control_device', 'weather_condition', 'lighting_condition',  \n",
    "        'trafficway_type', 'alignment', 'roadway_surface_cond',  \n",
    "        'road_defect', 'intersection_related_i',  'prim_contributory_cause'  \n",
    "    ]  \n",
    "    \n",
    "    # Add time_of_day if it exists  \n",
    "    if 'time_of_day' in df.columns:  \n",
    "        categorical_features.append('time_of_day')  \n",
    "    \n",
    "    numerical_features = [  \n",
    "        'num_units'  \n",
    "    ]  \n",
    "    \n",
    "    # Add temporal features if they exist  \n",
    "    temporal_features = ['crash_hour', 'crash_day_of_week', 'crash_month',   \n",
    "                       'is_weekend', 'is_rush_hour']  \n",
    "    \n",
    "    for feat in temporal_features:  \n",
    "        if feat in df.columns:  \n",
    "            numerical_features.append(feat)  \n",
    "    \n",
    "    # Optionally include injury features (remove if causing data leakage)  \n",
    "    # injury_features = [  \n",
    "    #     'injuries_total', 'injuries_fatal', 'injuries_incapacitating',  \n",
    "    #     'injuries_non_incapacitating', 'injuries_reported_not_evident',   \n",
    "    #     'injuries_no_indication'  \n",
    "    # ]  \n",
    "      \n",
    "    # for feat in injury_features:  \n",
    "    #     if feat in df.columns:  \n",
    "    #         numerical_features.append(feat)  \n",
    "    \n",
    "\n",
    "    # Make sure all categorical_features and numerical_features exist in df  \n",
    "    categorical_features = [f for f in categorical_features if f in df.columns]  \n",
    "    numerical_features = [f for f in numerical_features if f in df.columns]  \n",
    "    \n",
    "    print(f\"Using {len(categorical_features)} categorical features and {len(numerical_features)} numerical features\")  \n",
    "    \n",
    "    # 5. Return preprocessed data  \n",
    "    X = df[categorical_features + numerical_features]  \n",
    "    if 'target' in df.columns:  \n",
    "        y = df['target']  \n",
    "    else:  \n",
    "        y = None  # For prediction-only scenarios  \n",
    "    \n",
    "    return X, y, categorical_features, numerical_features  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5937",
   "metadata": {
    "papermill": {
     "duration": 0.003321,
     "end_time": "2025-04-05T16:54:35.080596",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.077275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**CREATE FOLDER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99b03b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.088057Z",
     "iopub.status.busy": "2025-04-05T16:54:35.087800Z",
     "iopub.status.idle": "2025-04-05T16:54:35.092502Z",
     "shell.execute_reply": "2025-04-05T16:54:35.091808Z"
    },
    "papermill": {
     "duration": 0.009629,
     "end_time": "2025-04-05T16:54:35.093585",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.083956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accident prediction system...\n"
     ]
    }
   ],
   "source": [
    "# Create output directories for models and plots  \n",
    "os.makedirs('output', exist_ok=True)  \n",
    "os.makedirs('output/models', exist_ok=True)  \n",
    "os.makedirs('output/plots', exist_ok=True)  \n",
    "os.makedirs('output/metrics', exist_ok=True)  \n",
    "\n",
    "print(\"Starting accident prediction system...\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f624ec",
   "metadata": {
    "papermill": {
     "duration": 0.003191,
     "end_time": "2025-04-05T16:54:35.100313",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.097122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " **SAVE & LOAD MODELS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fc1e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.107821Z",
     "iopub.status.busy": "2025-04-05T16:54:35.107585Z",
     "iopub.status.idle": "2025-04-05T16:54:35.114447Z",
     "shell.execute_reply": "2025-04-05T16:54:35.113832Z"
    },
    "papermill": {
     "duration": 0.011977,
     "end_time": "2025-04-05T16:54:35.115633",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.103656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_models(sklearn_models):  \n",
    "    \"\"\"  \n",
    "    Save all trained models to files  \n",
    "    \n",
    "    Args:  \n",
    "        sklearn_models: Dictionary with trained sklearn models  \n",
    "    \"\"\"  \n",
    "    print(\"\\nSaving models to files...\")  \n",
    "    \n",
    "    # Save sklearn models  \n",
    "    joblib.dump(sklearn_models['random_forest'], 'output/models/random_forest_model.pkl')  \n",
    "    joblib.dump(sklearn_models['logistic_regression'], 'output/models/logistic_regression_model.pkl')  \n",
    "    joblib.dump(sklearn_models['naive_bayes'], 'output/models/naive_bayes_model.pkl')  \n",
    "    \n",
    "    # Save model metadata  \n",
    "    metadata = {  \n",
    "        'models': ['random_forest', 'logistic_regression', 'naive_bayes', 'neural_network'],  \n",
    "        'sklearn_categorical_features': sklearn_models['categorical_features'],  \n",
    "        'sklearn_numerical_features': sklearn_models['numerical_features'],  \n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')  \n",
    "    }  \n",
    "    \n",
    "    with open('output/models/model_metadata.pkl', 'wb') as f:  \n",
    "        pickle.dump(metadata, f)  \n",
    "    \n",
    "    print(\"All models successfully saved!\")  \n",
    "\n",
    "def load_models():  \n",
    "    \"\"\"  \n",
    "    Load all trained models from files  \n",
    "    \n",
    "    Returns:  \n",
    "        Dictionary with loaded models  \n",
    "    \"\"\"  \n",
    "    print(\"\\nLoading models from files...\")  \n",
    "    \n",
    "    models = {}  \n",
    "    \n",
    "    try:  \n",
    "        # Load sklearn models  \n",
    "        models['random_forest'] = joblib.load('output/models/random_forest_model.pkl')  \n",
    "        models['logistic_regression'] = joblib.load('output/models/logistic_regression_model.pkl')  \n",
    "        models['naive_bayes'] = joblib.load('output/models/naive_bayes_model.pkl')  \n",
    "        \n",
    "        # Load PyTorch neural network  \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "        \n",
    "        try:  \n",
    "            # Try to load full model first  \n",
    "            models['neural_network'] = torch.load('output/models/full_nn_model.pt', map_location=device)  \n",
    "        except:  \n",
    "            # If that fails, load the preprocessing info and build the model  \n",
    "            with open('output/models/nn_preprocessing.pkl', 'rb') as f:  \n",
    "                preprocessing = pickle.load(f)  \n",
    "            \n",
    "            cat_dims = preprocessing['cat_dims']  \n",
    "            num_dims = len(preprocessing['numerical_features'])  \n",
    "            \n",
    "            # Create the model architecture and load weights  \n",
    "            models['neural_network'] = EmbeddingNet(cat_dims=cat_dims, num_dims=num_dims).to(device)  \n",
    "            models['neural_network'].load_state_dict(torch.load('output/models/best_nn_model.pt', map_location=device))  \n",
    "        \n",
    "        # Set the model to evaluation mode  \n",
    "        models['neural_network'].eval()  \n",
    "        \n",
    "        # Load model metadata  \n",
    "        with open('output/models/model_metadata.pkl', 'rb') as f:  \n",
    "            models['metadata'] = pickle.load(f)  \n",
    "        \n",
    "        print(\"All models successfully loaded!\")  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error loading models: {e}\")  \n",
    "    \n",
    "    return models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8e85e",
   "metadata": {
    "papermill": {
     "duration": 0.003233,
     "end_time": "2025-04-05T16:54:35.122261",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.119028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**RANDOM FOREST, LOGISTIC REGRESSION, NAIVE BAYES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457481ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.129725Z",
     "iopub.status.busy": "2025-04-05T16:54:35.129514Z",
     "iopub.status.idle": "2025-04-05T16:54:35.136315Z",
     "shell.execute_reply": "2025-04-05T16:54:35.135670Z"
    },
    "papermill": {
     "duration": 0.011906,
     "end_time": "2025-04-05T16:54:35.137472",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.125566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_sklearn_models(X, y, categorical_features, numerical_features):  \n",
    "    \"\"\"  \n",
    "    Train Random Forest, Logistic Regression, and Naive Bayes models  \n",
    "    \n",
    "    Args:  \n",
    "        X: Feature DataFrame  \n",
    "        y: Target Series  \n",
    "        categorical_features: List of categorical feature names  \n",
    "        numerical_features: List of numerical feature names  \n",
    "        \n",
    "    Returns:  \n",
    "        Dictionary of trained models and preprocessors  \n",
    "    \"\"\"  \n",
    "    # Train/test split  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "    \n",
    "    # Save train/test indices for consistency across models  \n",
    "    train_indices = X_train.index  \n",
    "    test_indices = X_test.index  \n",
    "    \n",
    "    indices_info = {  \n",
    "        'train_indices': train_indices,  \n",
    "        'test_indices': test_indices  \n",
    "    }  \n",
    "    \n",
    "    # Save indices for later use  \n",
    "    with open('output/models/train_test_indices.pkl', 'wb') as f:  \n",
    "        pickle.dump(indices_info, f)  \n",
    "    \n",
    "    # Create preprocessor  \n",
    "    preprocessor = ColumnTransformer(  \n",
    "        transformers=[  \n",
    "            ('cat', Pipeline([  \n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='UNKNOWN')),  \n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  \n",
    "            ]), categorical_features),  \n",
    "            ('num', Pipeline([  \n",
    "                ('imputer', SimpleImputer(strategy='median')),  \n",
    "                ('scaler', StandardScaler())  \n",
    "            ]), numerical_features)  \n",
    "        ]  \n",
    "    )  \n",
    "    \n",
    "    # 1. Random Forest  \n",
    "    print(\"\\nTraining Random Forest...\")  \n",
    "    rf_pipeline = Pipeline([  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', RandomForestClassifier(  \n",
    "            n_estimators=100,   \n",
    "            max_depth=15,  \n",
    "            min_samples_split=10,  \n",
    "            min_samples_leaf=5,  \n",
    "            class_weight='balanced',  \n",
    "            random_state=42  \n",
    "        ))  \n",
    "    ])  \n",
    "    \n",
    "    rf_pipeline.fit(X_train, y_train)  \n",
    "    \n",
    "    # 2. Logistic Regression  \n",
    "    print(\"Training Logistic Regression...\")  \n",
    "    lr_pipeline = Pipeline([  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', LogisticRegression(  \n",
    "            C=1.0,  \n",
    "            max_iter=1000,  \n",
    "            class_weight='balanced',  \n",
    "            random_state=42  \n",
    "        ))  \n",
    "    ])  \n",
    "    \n",
    "    lr_pipeline.fit(X_train, y_train)  \n",
    "    \n",
    "    # 3. Naive Bayes  \n",
    "    print(\"Training Naive Bayes...\")  \n",
    "    nb_pipeline = Pipeline([  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', GaussianNB())  \n",
    "    ])  \n",
    "    \n",
    "    nb_pipeline.fit(X_train, y_train)  \n",
    "    \n",
    "    # Save preprocessor separately for potential reuse  \n",
    "    joblib.dump(preprocessor, 'output/models/sklearn_preprocessor.pkl')  \n",
    "    \n",
    "    # Return models and test data  \n",
    "    return {  \n",
    "        'random_forest': rf_pipeline,  \n",
    "        'logistic_regression': lr_pipeline,  \n",
    "        'naive_bayes': nb_pipeline,  \n",
    "        'X_test': X_test,  \n",
    "        'y_test': y_test,  \n",
    "        'preprocessor': preprocessor,  \n",
    "        'categorical_features': categorical_features,  \n",
    "        'numerical_features': numerical_features  \n",
    "    }  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa3cb8",
   "metadata": {
    "papermill": {
     "duration": 0.003211,
     "end_time": "2025-04-05T16:54:35.144040",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.140829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39249eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.151922Z",
     "iopub.status.busy": "2025-04-05T16:54:35.151671Z",
     "iopub.status.idle": "2025-04-05T16:54:35.177635Z",
     "shell.execute_reply": "2025-04-05T16:54:35.177010Z"
    },
    "papermill": {
     "duration": 0.031438,
     "end_time": "2025-04-05T16:54:35.178840",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.147402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the residual block  \n",
    "class ResidualBlock(nn.Module):  \n",
    "    def __init__(self, in_features, out_features, dropout=0.3):  \n",
    "        super(ResidualBlock, self).__init__()  \n",
    "        self.block = nn.Sequential(  \n",
    "            nn.Linear(in_features, out_features),  \n",
    "            nn.BatchNorm1d(out_features),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(dropout),  \n",
    "            nn.Linear(out_features, out_features),  \n",
    "            nn.BatchNorm1d(out_features),  \n",
    "        )  \n",
    "        self.relu = nn.ReLU()  \n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        # If dimensions don't match, use a projection shortcut  \n",
    "        if in_features != out_features:  \n",
    "            self.shortcut = nn.Linear(in_features, out_features)  \n",
    "        else:  \n",
    "            self.shortcut = nn.Identity()  \n",
    "            \n",
    "    def forward(self, x):  \n",
    "        residual = self.shortcut(x)  \n",
    "        out = self.block(x)  \n",
    "        out += residual  \n",
    "        out = self.relu(out)  \n",
    "        out = self.dropout(out)  \n",
    "        return out  \n",
    "\n",
    "# Define a simple self-attention mechanism  \n",
    "class SelfAttention(nn.Module):  \n",
    "    def __init__(self, dim):  \n",
    "        super(SelfAttention, self).__init__()  \n",
    "        self.query = nn.Linear(dim, dim)  \n",
    "        self.key = nn.Linear(dim, dim)  \n",
    "        self.value = nn.Linear(dim, dim)  \n",
    "        self.scale = dim ** -0.5  \n",
    "        self.softmax = nn.Softmax(dim=-1)  \n",
    "        \n",
    "    def forward(self, x):  \n",
    "        # Reshape for attention if necessary  \n",
    "        original_shape = x.shape  \n",
    "        if len(original_shape) == 2:  \n",
    "            # For batched inputs without sequence dimension  \n",
    "            x = x.unsqueeze(1)  # Add sequence dimension  \n",
    "            \n",
    "        q = self.query(x)  \n",
    "        k = self.key(x)  \n",
    "        v = self.value(x)  \n",
    "        \n",
    "        # Compute attention scores  \n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale  \n",
    "        attn = self.softmax(attn)  \n",
    "        \n",
    "        # Apply attention to values  \n",
    "        out = torch.bmm(attn, v)  \n",
    "        \n",
    "        # Reshape back if needed  \n",
    "        if len(original_shape) == 2:  \n",
    "            out = out.squeeze(1)  \n",
    "            \n",
    "        return out  \n",
    "    \n",
    "class EmbeddingNet(nn.Module):  \n",
    "    \"\"\"  \n",
    "    PyTorch neural network model with embedding layers for categorical features  \n",
    "    \"\"\"  \n",
    "    def __init__(self, cat_dims, num_dims):  \n",
    "        \"\"\"  \n",
    "        Initialize the neural network  \n",
    "        \n",
    "        Args:  \n",
    "            cat_dims: List of dimensions for each categorical feature  \n",
    "            num_dims: Number of numerical features  \n",
    "        \"\"\"  \n",
    "        super(EmbeddingNet, self).__init__()  \n",
    "        \n",
    "        # Embedding layers for categorical features  \n",
    "        self.embeddings = nn.ModuleList()  \n",
    "        self.embedding_dims = []  \n",
    "        \n",
    "        for dim in cat_dims:  \n",
    "            # Rule of thumb for embedding size: min(50, (cardinality+1)//2)  \n",
    "            embed_dim = min(50, (dim+1)//2)  \n",
    "            self.embedding_dims.append(embed_dim)  \n",
    "            self.embeddings.append(nn.Embedding(dim+1, embed_dim))  \n",
    "        \n",
    "        # Calculate total input dimension after embeddings and numerical features  \n",
    "        total_embed_dim = sum(self.embedding_dims) + num_dims  \n",
    "        \n",
    "        # Deep layers  \n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.BatchNorm1d(total_embed_dim),  \n",
    "            nn.Linear(total_embed_dim, 256),  # Wider first layer  \n",
    "            nn.LeakyReLU(0.2),  # LeakyReLU instead of ReLU  \n",
    "            nn.Dropout(0.3),  \n",
    "            nn.BatchNorm1d(256),  \n",
    "            \n",
    "            # First residual block  \n",
    "            ResidualBlock(256, 256, dropout=0.3),  \n",
    "            \n",
    "            nn.Linear(256, 192),  \n",
    "            nn.GELU(),  # Using GELU activation  \n",
    "            nn.Dropout(0.4),  # Increased dropout  \n",
    "            nn.BatchNorm1d(192),  \n",
    "            \n",
    "            # Self-attention mechanism  \n",
    "            SelfAttention(192),  \n",
    "            \n",
    "            nn.Linear(192, 128),  \n",
    "            nn.SiLU(),  # Using SiLU (Swish) activation  \n",
    "            nn.Dropout(0.3),  \n",
    "            nn.BatchNorm1d(128),  \n",
    "            \n",
    "            # Second residual block  \n",
    "            ResidualBlock(128, 128, dropout=0.3),  \n",
    "            \n",
    "            nn.Linear(128, 96),  \n",
    "            nn.Mish(),  # Using Mish activation  \n",
    "            nn.Dropout(0.25),  \n",
    "            nn.BatchNorm1d(96),  \n",
    "            \n",
    "            nn.Linear(96, 64),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(0.2),  \n",
    "            nn.BatchNorm1d(64),  \n",
    "            \n",
    "            # Third residual block  \n",
    "            ResidualBlock(64, 64, dropout=0.2),  \n",
    "            \n",
    "            nn.Linear(64, 32),  \n",
    "            nn.ReLU(),  \n",
    "            nn.BatchNorm1d(32),  \n",
    "            nn.Dropout(0.2),  \n",
    "            \n",
    "            nn.Linear(32, 16),  \n",
    "            nn.ReLU(),  \n",
    "            nn.BatchNorm1d(16),  \n",
    "            \n",
    "            nn.Linear(16, 1),  \n",
    "            nn.Sigmoid()  \n",
    "        )  \n",
    "        \n",
    "    def forward(self, cat_x, num_x):  \n",
    "        \"\"\"  \n",
    "        Forward pass through the network  \n",
    "        \n",
    "        Args:  \n",
    "            cat_x: Categorical features (as a list of tensors)  \n",
    "            num_x: Numerical features  \n",
    "            \n",
    "        Returns:  \n",
    "            Model output (probability)  \n",
    "        \"\"\"  \n",
    "        # Process embeddings  \n",
    "        embeds = []  \n",
    "        for i, embedding_layer in enumerate(self.embeddings):  \n",
    "            embed = embedding_layer(cat_x[:, i])  \n",
    "            embeds.append(embed)  \n",
    "        \n",
    "        # Concatenate all embeddings and numerical features  \n",
    "        x = torch.cat([*embeds, num_x], dim=1)  \n",
    "        \n",
    "        # Feed through deep layers  \n",
    "        return self.layers(x)  \n",
    "\n",
    "def train_neural_network(X, y, categorical_features, numerical_features):  \n",
    "    \"\"\"  \n",
    "    Train PyTorch Neural Network model with embedding layers  \n",
    "    \n",
    "    Args:  \n",
    "        X: Feature DataFrame  \n",
    "        y: Target Series  \n",
    "        categorical_features: List of categorical feature names  \n",
    "        numerical_features: List of numerical feature names  \n",
    "        \n",
    "    Returns:  \n",
    "        Trained model and preprocessors  \n",
    "    \"\"\"  \n",
    "    print(\"\\nPreprocessing data for Neural Network...\")  \n",
    "    \n",
    "    # Set device (GPU if available, otherwise CPU)  \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "    print(f\"Using device: {device}\")  \n",
    "    \n",
    "    # Load train/test indices for consistency  \n",
    "    try:  \n",
    "        with open('output/models/train_test_indices.pkl', 'rb') as f:  \n",
    "            indices = pickle.load(f)  \n",
    "            train_indices = indices['train_indices']  \n",
    "            test_indices = indices['test_indices']  \n",
    "            \n",
    "        X_train = X.loc[train_indices]  \n",
    "        X_test = X.loc[test_indices]  \n",
    "        y_train = y.loc[train_indices]  \n",
    "        y_test = y.loc[test_indices]  \n",
    "        print(\"Using consistent train/test split with sklearn models\")  \n",
    "    except:  \n",
    "        # If indices file doesn't exist, create new split  \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "        print(\"Created new train/test split for neural network\")  \n",
    "    \n",
    "    # Preprocess categorical features (label encoding for embeddings)  \n",
    "    label_encoders = {}  \n",
    "    cat_encoded_df = pd.DataFrame(index=X.index)  \n",
    "    \n",
    "    for feature in categorical_features:  \n",
    "        le = LabelEncoder()  \n",
    "        cat_encoded_df[feature] = le.fit_transform(X[feature].astype(str))  \n",
    "        label_encoders[feature] = le  \n",
    "    \n",
    "    # Get cardinalities for embedding dimensions  \n",
    "    cat_dims = [len(label_encoders[col].classes_) for col in categorical_features]  \n",
    "    \n",
    "    # Preprocess numerical features  \n",
    "    scaler = StandardScaler()  \n",
    "    num_df = pd.DataFrame(scaler.fit_transform(X[numerical_features]),   \n",
    "                          columns=numerical_features,  \n",
    "                          index=X.index)  \n",
    "    \n",
    "    # Prepare PyTorch tensors  \n",
    "    X_cat_train = torch.tensor(cat_encoded_df.loc[train_indices].values, dtype=torch.long).to(device)  \n",
    "    X_cat_test = torch.tensor(cat_encoded_df.loc[test_indices].values, dtype=torch.long).to(device)  \n",
    "    X_num_train = torch.tensor(num_df.loc[train_indices].values, dtype=torch.float32).to(device)  \n",
    "    X_num_test = torch.tensor(num_df.loc[test_indices].values, dtype=torch.float32).to(device)  \n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)  \n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)  \n",
    "    \n",
    "    # Create dataset and dataloader  \n",
    "    train_dataset = TensorDataset(X_cat_train, X_num_train, y_train_tensor)  \n",
    "    test_dataset = TensorDataset(X_cat_test, X_num_test, y_test_tensor)  \n",
    "    \n",
    "    batch_size = 64  \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)  \n",
    "    \n",
    "    # Create model  \n",
    "    print(\"Building Neural Network model...\")  \n",
    "    model = EmbeddingNet(cat_dims=cat_dims, num_dims=X_num_train.shape[1]).to(device)  \n",
    "    print(model)  \n",
    "    \n",
    "    # Loss and optimizer  \n",
    "    criterion = nn.BCELoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "    \n",
    "    # Calculate class weights for imbalanced data  \n",
    "    if sum(y_train) / len(y_train) < 0.5:  \n",
    "        pos_weight = torch.tensor((len(y_train) - sum(y_train)) / sum(y_train)).to(device)  \n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  \n",
    "    \n",
    "    # Training loop  \n",
    "    print(\"Training Neural Network...\")  \n",
    "    epochs = 30  \n",
    "    train_losses = []  \n",
    "    val_losses = []  \n",
    "    train_aucs = []  \n",
    "    val_aucs = []  \n",
    "    best_val_auc = 0  \n",
    "    patience = 10  \n",
    "    patience_counter = 0  \n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        # Training  \n",
    "        model.train()  \n",
    "        train_loss = 0  \n",
    "        train_preds = []  \n",
    "        train_true = []  \n",
    "        \n",
    "        for cat_data, num_data, targets in train_loader:  \n",
    "            # Zero the gradients  \n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            # Forward pass  \n",
    "            outputs = model(cat_data, num_data)  \n",
    "            \n",
    "            # Calculate loss  \n",
    "            loss = criterion(outputs, targets)  \n",
    "            \n",
    "            # Backward pass and optimize  \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            \n",
    "            # Record loss and predictions  \n",
    "            train_loss += loss.item() * len(targets)  \n",
    "            train_preds.extend(outputs.detach().cpu().numpy())  \n",
    "            train_true.extend(targets.detach().cpu().numpy())  \n",
    "        \n",
    "        train_loss /= len(train_dataset)  \n",
    "        train_auc = roc_auc_score(train_true, train_preds)  \n",
    "        train_losses.append(train_loss)  \n",
    "        train_aucs.append(train_auc)  \n",
    "        \n",
    "        # Validation  \n",
    "        model.eval()  \n",
    "        val_loss = 0  \n",
    "        val_preds = []  \n",
    "        val_true = []  \n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            for cat_data, num_data, targets in test_loader:  \n",
    "                outputs = model(cat_data, num_data)  \n",
    "                loss = criterion(outputs, targets)  \n",
    "                \n",
    "                val_loss += loss.item() * len(targets)  \n",
    "                val_preds.extend(outputs.detach().cpu().numpy())  \n",
    "                val_true.extend(targets.detach().cpu().numpy())  \n",
    "        \n",
    "        val_loss /= len(test_dataset)  \n",
    "        val_auc = roc_auc_score(val_true, val_preds)  \n",
    "        val_losses.append(val_loss)  \n",
    "        val_aucs.append(val_auc)  \n",
    "        \n",
    "        # Print progress  \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")  \n",
    "        \n",
    "        # Check for improvement  \n",
    "        if val_auc > best_val_auc:  \n",
    "            best_val_auc = val_auc  \n",
    "            patience_counter = 0  \n",
    "            # Save best model  \n",
    "            torch.save(model.state_dict(), 'output/models/best_nn_model.pt')  \n",
    "            print(f\"Improved! Saved model with validation AUC: {val_auc:.4f}\")  \n",
    "        else:  \n",
    "            patience_counter += 1  \n",
    "            if patience_counter >= patience:  \n",
    "                print(f\"Early stopping after {epoch+1} epochs\")  \n",
    "                break  \n",
    "    \n",
    "    # Load best model  \n",
    "    model.load_state_dict(torch.load('output/models/best_nn_model.pt'))  \n",
    "    \n",
    "    # Always save the final model in addition to the best model  \n",
    "    torch.save(model.state_dict(), 'output/models/final_nn_model.pt')  \n",
    "    \n",
    "    # Save the full model (architecture + weights)  \n",
    "    torch.save(model, 'output/models/full_nn_model.pt')  \n",
    "    \n",
    "    # Save model configuration and preprocessing objects  \n",
    "    nn_preprocessing = {  \n",
    "        'label_encoders': label_encoders,  \n",
    "        'scaler': scaler,  \n",
    "        'categorical_features': categorical_features,  \n",
    "        'numerical_features': numerical_features,  \n",
    "        'cat_dims': cat_dims,  \n",
    "        'device': str(device),  \n",
    "        'model_architecture': {  \n",
    "            'cat_dims': cat_dims,  \n",
    "            'num_dims': X_num_train.shape[1]  \n",
    "        }  \n",
    "    }  \n",
    "    \n",
    "    with open('output/models/nn_preprocessing.pkl', 'wb') as f:  \n",
    "        pickle.dump(nn_preprocessing, f)  \n",
    "    \n",
    "    # Save training history  \n",
    "    history = {  \n",
    "        'train_loss': train_losses,  \n",
    "        'val_loss': val_losses,  \n",
    "        'train_auc': train_aucs,  \n",
    "        'val_auc': val_aucs  \n",
    "    }  \n",
    "    \n",
    "    with open('output/models/nn_training_history.pkl', 'wb') as f:  \n",
    "        pickle.dump(history, f)  \n",
    "    \n",
    "    return {  \n",
    "        'model': model,  \n",
    "        'history': history,  \n",
    "        'X_cat_test': X_cat_test,  \n",
    "        'X_num_test': X_num_test,  \n",
    "        'y_test': y_test,  \n",
    "        'y_test_tensor': y_test_tensor,  \n",
    "        'label_encoders': label_encoders,  \n",
    "        'scaler': scaler,  \n",
    "        'categorical_features': categorical_features,  \n",
    "        'numerical_features': numerical_features,  \n",
    "        'device': device  \n",
    "    }  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04a9e0",
   "metadata": {
    "papermill": {
     "duration": 0.003251,
     "end_time": "2025-04-05T16:54:35.185699",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.182448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332ba5ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.193427Z",
     "iopub.status.busy": "2025-04-05T16:54:35.193203Z",
     "iopub.status.idle": "2025-04-05T16:54:35.221575Z",
     "shell.execute_reply": "2025-04-05T16:54:35.220937Z"
    },
    "papermill": {
     "duration": 0.033674,
     "end_time": "2025-04-05T16:54:35.222739",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.189065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_sklearn_models(models_dict):  \n",
    "    \"\"\"  \n",
    "    Evaluate the trained sklearn models  \n",
    "    \n",
    "    Args:  \n",
    "        models_dict: Dictionary with trained models and test data  \n",
    "        \n",
    "    Returns:  \n",
    "        Dictionary with evaluation metrics  \n",
    "    \"\"\"  \n",
    "    results = {}  \n",
    "    X_test = models_dict['X_test']  \n",
    "    y_test = models_dict['y_test']  \n",
    "    \n",
    "    # Models to evaluate  \n",
    "    models = {  \n",
    "        'Random Forest': models_dict['random_forest'],  \n",
    "        'Logistic Regression': models_dict['logistic_regression'],  \n",
    "        'Naive Bayes': models_dict['naive_bayes']  \n",
    "    }  \n",
    "    \n",
    "    # Prepare results summary for CSV export  \n",
    "    results_summary = []  \n",
    "    \n",
    "    # Evaluate each model  \n",
    "    for name, model in models.items():  \n",
    "        print(f\"\\nEvaluating {name}...\")  \n",
    "        \n",
    "        # Predictions  \n",
    "        y_pred = model.predict(X_test)  \n",
    "        y_proba = model.predict_proba(X_test)[:, 1]  \n",
    "        \n",
    "        # Metrics  \n",
    "        accuracy = (y_pred == y_test).mean()  \n",
    "        auc = roc_auc_score(y_test, y_proba)  \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)  \n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")  \n",
    "        print(f\"ROC AUC: {auc:.4f}\")  \n",
    "        print(\"Classification Report:\")  \n",
    "        print(classification_report(y_test, y_pred))  \n",
    "        \n",
    "        # Add to results summary  \n",
    "        results_summary.append({  \n",
    "            'model': name,  \n",
    "            'accuracy': accuracy,  \n",
    "            'auc': auc,  \n",
    "            'precision': report['weighted avg']['precision'],  \n",
    "            'recall': report['weighted avg']['recall'],  \n",
    "            'f1': report['weighted avg']['f1-score']  \n",
    "        })  \n",
    "        \n",
    "        # Plot confusion matrix  \n",
    "        cm = confusion_matrix(y_test, y_pred)  \n",
    "        plt.figure(figsize=(8, 6))  \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  \n",
    "                   xticklabels=['No Injury', 'Injury/Tow'],  \n",
    "                   yticklabels=['No Injury', 'Injury/Tow'])  \n",
    "        plt.xlabel('Predicted')  \n",
    "        plt.ylabel('Actual')  \n",
    "        plt.title(f'Confusion Matrix - {name}')  \n",
    "        plt.tight_layout()  \n",
    "        plt.savefig(f'output/plots/cm_{name.lower().replace(\" \", \"_\")}.png')  \n",
    "        plt.close()  \n",
    "        \n",
    "        # Plot ROC curve  \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)  \n",
    "        plt.figure(figsize=(8, 6))  \n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')  \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')  \n",
    "        plt.xlabel('False Positive Rate')  \n",
    "        plt.ylabel('True Positive Rate')  \n",
    "        plt.title(f'ROC Curve - {name}')  \n",
    "        plt.legend()  \n",
    "        plt.tight_layout()  \n",
    "        plt.savefig(f'output/plots/roc_{name.lower().replace(\" \", \"_\")}.png')  \n",
    "        plt.close()  \n",
    "        \n",
    "        # Store results  \n",
    "        results[name] = {  \n",
    "            'accuracy': accuracy,  \n",
    "            'auc': auc,  \n",
    "            'report': report,  \n",
    "            'y_pred': y_pred,  \n",
    "            'y_proba': y_proba  \n",
    "        }  \n",
    "    \n",
    "    # Save results summary to CSV  \n",
    "    pd.DataFrame(results_summary).to_csv('output/metrics/sklearn_models_results.csv', index=False)  \n",
    "    \n",
    "    return results  \n",
    "\n",
    "def evaluate_neural_network(nn_dict):  \n",
    "    \"\"\"  \n",
    "    Evaluate the trained neural network model  \n",
    "    \n",
    "    Args:  \n",
    "        nn_dict: Dictionary with neural network model and related data  \n",
    "        \n",
    "    Returns:  \n",
    "        Dictionary with evaluation metrics  \n",
    "    \"\"\"  \n",
    "    print(\"\\nEvaluating Neural Network...\")  \n",
    "    \n",
    "    # Unpack dictionary  \n",
    "    model = nn_dict['model']  \n",
    "    history = nn_dict['history']  \n",
    "    X_cat_test = nn_dict['X_cat_test']  \n",
    "    X_num_test = nn_dict['X_num_test']  \n",
    "    y_test = nn_dict['y_test']  \n",
    "    device = nn_dict['device']  \n",
    "    \n",
    "    # Evaluation mode  \n",
    "    model.eval()  \n",
    "    \n",
    "    # Make predictions  \n",
    "    with torch.no_grad():  \n",
    "        y_pred_proba = model(X_cat_test, X_num_test).cpu().numpy()  \n",
    "    \n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)  \n",
    "    \n",
    "    # Calculate metrics  \n",
    "    accuracy = (y_pred.flatten() == y_test.values).mean()  \n",
    "    auc = roc_auc_score(y_test, y_pred_proba)  \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)  \n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")  \n",
    "    print(f\"ROC AUC: {auc:.4f}\")  \n",
    "    print(\"Classification Report:\")  \n",
    "    print(classification_report(y_test, y_pred))  \n",
    "    \n",
    "    # Save metrics to CSV  \n",
    "    results_summary = [{  \n",
    "        'model': 'Neural Network',  \n",
    "        'accuracy': accuracy,  \n",
    "        'auc': auc,  \n",
    "        'precision': report['weighted avg']['precision'],  \n",
    "        'recall': report['weighted avg']['recall'],  \n",
    "        'f1': report['weighted avg']['f1-score']  \n",
    "    }]  \n",
    "    \n",
    "    pd.DataFrame(results_summary).to_csv('output/metrics/neural_network_results.csv', index=False)  \n",
    "    \n",
    "    # Plot training history  \n",
    "    plt.figure(figsize=(12, 5))  \n",
    "    plt.subplot(1, 2, 1)  \n",
    "    plt.plot(history['train_loss'], label='Training Loss')  \n",
    "    plt.plot(history['val_loss'], label='Validation Loss')  \n",
    "    plt.title('Training and Validation Loss')  \n",
    "    plt.xlabel('Epoch')  \n",
    "    plt.ylabel('Loss')  \n",
    "    plt.legend()  \n",
    "    \n",
    "    plt.subplot(1, 2, 2)  \n",
    "    plt.plot(history['train_auc'], label='Training AUC')  \n",
    "    plt.plot(history['val_auc'], label='Validation AUC')  \n",
    "    plt.title('Training and Validation AUC')  \n",
    "    plt.xlabel('Epoch')  \n",
    "    plt.ylabel('AUC')  \n",
    "    plt.legend()  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig('output/plots/nn_training_history.png')  \n",
    "    plt.close()  \n",
    "    \n",
    "    # Plot confusion matrix  \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)  \n",
    "    plt.figure(figsize=(8, 6))  \n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',  \n",
    "               xticklabels=['No Injury', 'Injury/Tow'],  \n",
    "               yticklabels=['No Injury', 'Injury/Tow'])  \n",
    "    plt.xlabel('Predicted')  \n",
    "    plt.ylabel('Actual')  \n",
    "    plt.title('Confusion Matrix - Neural Network')  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig('output/plots/cm_neural_network.png')  \n",
    "    plt.close()  \n",
    "    \n",
    "    # Plot ROC curve  \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)  \n",
    "    plt.figure(figsize=(8, 6))  \n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')  \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')  \n",
    "    plt.xlabel('False Positive Rate')  \n",
    "    plt.ylabel('True Positive Rate')  \n",
    "    plt.title('ROC Curve - Neural Network')  \n",
    "    plt.legend()  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig('output/plots/roc_neural_network.png')  \n",
    "    plt.close()  \n",
    "    \n",
    "    return {  \n",
    "        'accuracy': accuracy,  \n",
    "        'auc': auc,  \n",
    "        'report': report,  \n",
    "        'y_pred': y_pred.flatten(),  \n",
    "        'y_proba': y_pred_proba.flatten()  \n",
    "    }  \n",
    "\n",
    "def compare_models(sklearn_results, nn_results, y_test):  \n",
    "    \"\"\"  \n",
    "    Compare the performance of all models  \n",
    "    \n",
    "    Args:  \n",
    "        sklearn_results: Results from sklearn models  \n",
    "        nn_results: Results from neural network  \n",
    "        y_test: Test target values  \n",
    "    \"\"\"  \n",
    "    # Combine all results  \n",
    "    all_results = {**sklearn_results, 'Neural Network': nn_results}  \n",
    "    \n",
    "    # Extract metrics for comparison  \n",
    "    models = list(all_results.keys())  \n",
    "    accuracies = [all_results[model]['accuracy'] for model in models]  \n",
    "    aucs = [all_results[model]['auc'] for model in models]  \n",
    "    \n",
    "    # Create comparison DataFrame  \n",
    "    comparison_df = pd.DataFrame({  \n",
    "        'Model': models,  \n",
    "        'Accuracy': accuracies,  \n",
    "        'ROC AUC': aucs  \n",
    "    })  \n",
    "    \n",
    "    # Add precision, recall, and F1 score  \n",
    "    for model in models:  \n",
    "        report = all_results[model]['report']  \n",
    "        comparison_df.loc[comparison_df['Model'] == model, 'Precision'] = report['weighted avg']['precision']  \n",
    "        comparison_df.loc[comparison_df['Model'] == model, 'Recall'] = report['weighted avg']['recall']  \n",
    "        comparison_df.loc[comparison_df['Model'] == model, 'F1 Score'] = report['weighted avg']['f1-score']  \n",
    "    \n",
    "    # Sort by AUC  \n",
    "    comparison_df = comparison_df.sort_values('ROC AUC', ascending=False)  \n",
    "    \n",
    "    # Save comparison to CSV  \n",
    "    comparison_df.to_csv('output/metrics/model_comparison.csv', index=False)  \n",
    "    \n",
    "    # Create comparison bar chart  \n",
    "    plt.figure(figsize=(12, 6))  \n",
    "    \n",
    "    x = np.arange(len(models))  \n",
    "    width = 0.3  \n",
    "    \n",
    "    plt.bar(x - width, accuracies, width, label='Accuracy')  \n",
    "    plt.bar(x, aucs, width, label='ROC AUC')  \n",
    "    plt.bar(x + width, [all_results[model]['report']['weighted avg']['f1-score'] for model in models],   \n",
    "            width, label='F1 Score')  \n",
    "    \n",
    "    plt.xlabel('Models')  \n",
    "    plt.ylabel('Score')  \n",
    "    plt.title('Model Performance Comparison')  \n",
    "    plt.xticks(x, models)  \n",
    "    plt.legend()  \n",
    "    \n",
    "    # Add value labels  \n",
    "    for i, v in enumerate(accuracies):  \n",
    "        plt.text(i - width, v + 0.01, f'{v:.3f}', ha='center')  \n",
    "    \n",
    "    for i, v in enumerate(aucs):  \n",
    "        plt.text(i, v + 0.01, f'{v:.3f}', ha='center')  \n",
    "    \n",
    "    for i, v in enumerate([all_results[model]['report']['weighted avg']['f1-score'] for model in models]):  \n",
    "        plt.text(i + width, v + 0.01, f'{v:.3f}', ha='center')  \n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig('output/plots/model_comparison.png')  \n",
    "    plt.close()  \n",
    "    \n",
    "    # Create a comprehensive ROC curve comparison  \n",
    "    plt.figure(figsize=(10, 8))  \n",
    "    \n",
    "    for model_name in models:  \n",
    "        model_results = all_results[model_name]  \n",
    "        y_proba = model_results['y_proba']  \n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)  \n",
    "        auc = roc_auc_score(y_test, y_proba)  \n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')  \n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')  \n",
    "    plt.xlabel('False Positive Rate')  \n",
    "    plt.ylabel('True Positive Rate')  \n",
    "    plt.title('ROC Curve Comparison')  \n",
    "    plt.legend()  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig('output/plots/roc_comparison.png')  \n",
    "    plt.close()  \n",
    "    \n",
    "    # Print comparison table  \n",
    "    print(\"\\nModel Performance Comparison:\")  \n",
    "    print(comparison_df)  \n",
    "\n",
    "def analyze_feature_importance(rf_model, categorical_features, numerical_features):  \n",
    "    \"\"\"  \n",
    "    Analyze feature importance from Random Forest model  \n",
    "    \n",
    "    Args:  \n",
    "        rf_model: Trained Random Forest pipeline  \n",
    "        categorical_features: List of categorical feature names  \n",
    "        numerical_features: List of numerical feature names  \n",
    "    \"\"\"  \n",
    "    # Extract the preprocessor and classifier  \n",
    "    preprocessor = rf_model.named_steps['preprocessor']  \n",
    "    rf_classifier = rf_model.named_steps['classifier']  \n",
    "    \n",
    "    # Get feature names after preprocessing  \n",
    "    cat_features = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_features)  \n",
    "    all_features = np.append(cat_features, numerical_features)  \n",
    "    \n",
    "    # Get feature importances  \n",
    "    importances = rf_classifier.feature_importances_  \n",
    "    \n",
    "    # Sort features by importance  \n",
    "    indices = np.argsort(importances)[::-1]  \n",
    "    \n",
    "    # Create feature importance DataFrame  \n",
    "    importance_df = pd.DataFrame({  \n",
    "        'feature': [all_features[i] for i in indices],  \n",
    "        'importance': importances[indices]  \n",
    "    })  \n",
    "    \n",
    "    # Save to CSV  \n",
    "    importance_df.to_csv('output/metrics/feature_importance.csv', index=False)  \n",
    "    \n",
    "    # Plot top 20 features  \n",
    "    plt.figure(figsize=(12, 8))  \n",
    "    plt.title('Feature Importance (Random Forest)')  \n",
    "    plt.bar(range(min(20, len(all_features))),   \n",
    "            importances[indices[:20]],   \n",
    "            align='center')  \n",
    "    plt.xticks(range(min(20, len(all_features))),   \n",
    "              [all_features[i] for i in indices[:20]],   \n",
    "              rotation=90)  \n",
    "    plt.tight_layout()  \n",
    "    plt.savefig('output/plots/feature_importance.png')  \n",
    "    plt.close()  \n",
    "    \n",
    "    # Print top 10 features  \n",
    "    print(\"\\nTop 10 Most Important Features:\")  \n",
    "    for i in range(min(10, len(all_features))):  \n",
    "        feature_idx = indices[i]  \n",
    "        print(f\"{all_features[feature_idx]}: {importances[feature_idx]:.4f}\")  \n",
    "\n",
    "# ==================  \n",
    "# PREDICTION FUNCTIONS  \n",
    "# ==================  \n",
    "\n",
    "def predict_with_sklearn_model(model, new_data):  \n",
    "    \"\"\"  \n",
    "    Make prediction with a sklearn model  \n",
    "    \n",
    "    Args:  \n",
    "        model: Trained sklearn pipeline  \n",
    "        new_data: New data for prediction (DataFrame)  \n",
    "        \n",
    "    Returns:  \n",
    "        Probability of accident  \n",
    "    \"\"\"  \n",
    "    return model.predict_proba(new_data)[0, 1]  \n",
    "\n",
    "def predict_with_pytorch_network(new_data):  \n",
    "    \"\"\"  \n",
    "    Make prediction with the PyTorch neural network model  \n",
    "    \n",
    "    Args:  \n",
    "        new_data: New data for prediction (DataFrame or dict)  \n",
    "        \n",
    "    Returns:  \n",
    "        Probability of accident  \n",
    "    \"\"\"  \n",
    "    # Load the model and preprocessing objects  \n",
    "    try:  \n",
    "        # Load preprocessing info  \n",
    "        with open('output/models/nn_preprocessing.pkl', 'rb') as f:  \n",
    "            preprocessing = pickle.load(f)  \n",
    "            \n",
    "        label_encoders = preprocessing['label_encoders']  \n",
    "        scaler = preprocessing['scaler']  \n",
    "        categorical_features = preprocessing['categorical_features']  \n",
    "        numerical_features = preprocessing['numerical_features']  \n",
    "        cat_dims = preprocessing['cat_dims']  \n",
    "        num_dims = len(numerical_features)  \n",
    "        \n",
    "        # Set device  \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "        \n",
    "        # Load model  \n",
    "        try:  \n",
    "            # Try to load full model first  \n",
    "            model = torch.load('output/models/full_nn_model.pt', map_location=device)  \n",
    "        except:  \n",
    "            # If full model not available, load architecture and weights separately  \n",
    "            model = EmbeddingNet(cat_dims=cat_dims, num_dims=num_dims).to(device)  \n",
    "            model.load_state_dict(torch.load('output/models/best_nn_model.pt', map_location=device))  \n",
    "        \n",
    "        model.eval()  \n",
    "        \n",
    "        # Convert dict to DataFrame if needed  \n",
    "        if isinstance(new_data, dict):  \n",
    "            new_data = pd.DataFrame([new_data])  \n",
    "        \n",
    "        # Preprocess categorical features  \n",
    "        cat_processed = []  \n",
    "        for feature in categorical_features:  \n",
    "            le = label_encoders[feature]  \n",
    "            value = new_data[feature].iloc[0]  \n",
    "            try:  \n",
    "                encoded = le.transform([str(value)])[0]  \n",
    "            except:  \n",
    "                # Handle unseen categories  \n",
    "                encoded = 0  # Default to first category  \n",
    "            cat_processed.append(encoded)  \n",
    "        \n",
    "        # Preprocess numerical features  \n",
    "        num_values = []  \n",
    "        for feature in numerical_features:  \n",
    "            value = new_data[feature].iloc[0]  \n",
    "            num_values.append(value)  \n",
    "        \n",
    "        num_processed = scaler.transform(np.array([num_values]))  \n",
    "        \n",
    "        # Convert to PyTorch tensors  \n",
    "        cat_tensor = torch.tensor([cat_processed], dtype=torch.long).to(device)  \n",
    "        num_tensor = torch.tensor(num_processed, dtype=torch.float32).to(device)  \n",
    "        \n",
    "        # Make prediction  \n",
    "        with torch.no_grad():  \n",
    "            prediction = model(cat_tensor, num_tensor).item()  \n",
    "            \n",
    "        return prediction  \n",
    "    \n",
    "    except Exception as e:  \n",
    "        print(f\"Error making prediction with PyTorch neural network: {e}\")  \n",
    "        return None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7898e7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:54:35.230213Z",
     "iopub.status.busy": "2025-04-05T16:54:35.230016Z",
     "iopub.status.idle": "2025-04-05T17:05:22.193731Z",
     "shell.execute_reply": "2025-04-05T17:05:22.192844Z"
    },
    "papermill": {
     "duration": 646.969077,
     "end_time": "2025-04-05T17:05:22.195238",
     "exception": false,
     "start_time": "2025-04-05T16:54:35.226161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "Target distribution: {0: 117376, 1: 91930}\n",
      "Using 9 categorical features and 4 numerical features\n",
      "\n",
      "Training Random Forest...\n",
      "Training Logistic Regression...\n",
      "Training Naive Bayes...\n",
      "\n",
      "Saving models to files...\n",
      "All models successfully saved!\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Accuracy: 0.6677\n",
      "ROC AUC: 0.7265\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.69     23350\n",
      "           1       0.62      0.66      0.64     18512\n",
      "\n",
      "    accuracy                           0.67     41862\n",
      "   macro avg       0.66      0.67      0.67     41862\n",
      "weighted avg       0.67      0.67      0.67     41862\n",
      "\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "Accuracy: 0.6582\n",
      "ROC AUC: 0.7156\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68     23350\n",
      "           1       0.60      0.67      0.63     18512\n",
      "\n",
      "    accuracy                           0.66     41862\n",
      "   macro avg       0.66      0.66      0.66     41862\n",
      "weighted avg       0.66      0.66      0.66     41862\n",
      "\n",
      "\n",
      "Evaluating Naive Bayes...\n",
      "Accuracy: 0.6511\n",
      "ROC AUC: 0.6923\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71     23350\n",
      "           1       0.63      0.53      0.57     18512\n",
      "\n",
      "    accuracy                           0.65     41862\n",
      "   macro avg       0.65      0.64      0.64     41862\n",
      "weighted avg       0.65      0.65      0.65     41862\n",
      "\n",
      "\n",
      "Preprocessing data for Neural Network...\n",
      "Using device: cuda:0\n",
      "Using consistent train/test split with sklearn models\n",
      "Building Neural Network model...\n",
      "EmbeddingNet(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(20, 10)\n",
      "    (1): Embedding(13, 6)\n",
      "    (2): Embedding(7, 3)\n",
      "    (3): Embedding(21, 10)\n",
      "    (4): Embedding(7, 3)\n",
      "    (5-6): 2 x Embedding(8, 4)\n",
      "    (7): Embedding(3, 1)\n",
      "    (8): Embedding(41, 20)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): BatchNorm1d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Linear(in_features=65, out_features=256, bias=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "        (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (6): Linear(in_features=256, out_features=192, bias=True)\n",
      "    (7): GELU(approximate='none')\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): SelfAttention(\n",
      "      (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (11): Linear(in_features=192, out_features=128, bias=True)\n",
      "    (12): SiLU()\n",
      "    (13): Dropout(p=0.3, inplace=False)\n",
      "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "        (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (16): Linear(in_features=128, out_features=96, bias=True)\n",
      "    (17): Mish()\n",
      "    (18): Dropout(p=0.25, inplace=False)\n",
      "    (19): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): Linear(in_features=96, out_features=64, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): Dropout(p=0.2, inplace=False)\n",
      "    (23): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (shortcut): Identity()\n",
      "    )\n",
      "    (25): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (26): ReLU()\n",
      "    (27): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): Dropout(p=0.2, inplace=False)\n",
      "    (29): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (30): ReLU()\n",
      "    (31): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (33): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Training Neural Network...\n",
      "Epoch 1/30 - Train Loss: 0.7498, Train AUC: 0.6658, Val Loss: 0.7411, Val AUC: 0.7137\n",
      "Improved! Saved model with validation AUC: 0.7137\n",
      "Epoch 2/30 - Train Loss: 0.7402, Train AUC: 0.6927, Val Loss: 0.7379, Val AUC: 0.7196\n",
      "Improved! Saved model with validation AUC: 0.7196\n",
      "Epoch 3/30 - Train Loss: 0.7388, Train AUC: 0.6944, Val Loss: 0.7369, Val AUC: 0.7194\n",
      "Epoch 4/30 - Train Loss: 0.7379, Train AUC: 0.6963, Val Loss: 0.7367, Val AUC: 0.7114\n",
      "Epoch 5/30 - Train Loss: 0.7374, Train AUC: 0.7014, Val Loss: 0.7353, Val AUC: 0.7215\n",
      "Improved! Saved model with validation AUC: 0.7215\n",
      "Epoch 6/30 - Train Loss: 0.7370, Train AUC: 0.6994, Val Loss: 0.7365, Val AUC: 0.7186\n",
      "Epoch 7/30 - Train Loss: 0.7367, Train AUC: 0.6990, Val Loss: 0.7351, Val AUC: 0.7215\n",
      "Improved! Saved model with validation AUC: 0.7215\n",
      "Epoch 8/30 - Train Loss: 0.7366, Train AUC: 0.6946, Val Loss: 0.7361, Val AUC: 0.7170\n",
      "Epoch 9/30 - Train Loss: 0.7362, Train AUC: 0.7048, Val Loss: 0.7358, Val AUC: 0.7198\n",
      "Epoch 10/30 - Train Loss: 0.7363, Train AUC: 0.6980, Val Loss: 0.7365, Val AUC: 0.7220\n",
      "Improved! Saved model with validation AUC: 0.7220\n",
      "Epoch 11/30 - Train Loss: 0.7360, Train AUC: 0.7056, Val Loss: 0.7356, Val AUC: 0.7210\n",
      "Epoch 12/30 - Train Loss: 0.7358, Train AUC: 0.7047, Val Loss: 0.7358, Val AUC: 0.7215\n",
      "Epoch 13/30 - Train Loss: 0.7358, Train AUC: 0.7086, Val Loss: 0.7349, Val AUC: 0.7228\n",
      "Improved! Saved model with validation AUC: 0.7228\n",
      "Epoch 14/30 - Train Loss: 0.7358, Train AUC: 0.7126, Val Loss: 0.7342, Val AUC: 0.7259\n",
      "Improved! Saved model with validation AUC: 0.7259\n",
      "Epoch 15/30 - Train Loss: 0.7350, Train AUC: 0.7086, Val Loss: 0.7347, Val AUC: 0.7230\n",
      "Epoch 16/30 - Train Loss: 0.7354, Train AUC: 0.7088, Val Loss: 0.7342, Val AUC: 0.7238\n",
      "Epoch 17/30 - Train Loss: 0.7355, Train AUC: 0.7131, Val Loss: 0.7353, Val AUC: 0.7247\n",
      "Epoch 18/30 - Train Loss: 0.7352, Train AUC: 0.7110, Val Loss: 0.7346, Val AUC: 0.7252\n",
      "Epoch 19/30 - Train Loss: 0.7352, Train AUC: 0.7077, Val Loss: 0.7358, Val AUC: 0.7199\n",
      "Epoch 20/30 - Train Loss: 0.7350, Train AUC: 0.7135, Val Loss: 0.7351, Val AUC: 0.7238\n",
      "Epoch 21/30 - Train Loss: 0.7351, Train AUC: 0.7090, Val Loss: 0.7348, Val AUC: 0.7218\n",
      "Epoch 22/30 - Train Loss: 0.7348, Train AUC: 0.7088, Val Loss: 0.7343, Val AUC: 0.7237\n",
      "Epoch 23/30 - Train Loss: 0.7347, Train AUC: 0.7107, Val Loss: 0.7346, Val AUC: 0.7221\n",
      "Epoch 24/30 - Train Loss: 0.7351, Train AUC: 0.7048, Val Loss: 0.7348, Val AUC: 0.7218\n",
      "Early stopping after 24 epochs\n",
      "\n",
      "Evaluating Neural Network...\n",
      "Accuracy: 0.6604\n",
      "ROC AUC: 0.7259\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74     23350\n",
      "           1       0.70      0.41      0.51     18512\n",
      "\n",
      "    accuracy                           0.66     41862\n",
      "   macro avg       0.67      0.63      0.63     41862\n",
      "weighted avg       0.67      0.66      0.64     41862\n",
      "\n",
      "\n",
      "Model Performance Comparison:\n",
      "                 Model  Accuracy   ROC AUC  Precision    Recall  F1 Score\n",
      "0        Random Forest  0.667718  0.726496   0.670480  0.667718  0.668552\n",
      "3       Neural Network  0.660408  0.725898   0.670095  0.660408  0.639689\n",
      "1  Logistic Regression  0.658163  0.715608   0.663153  0.658163  0.659263\n",
      "2          Naive Bayes  0.651092  0.692320   0.648221  0.651092  0.646335\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "num_units: 0.1509\n",
      "prim_contributory_cause_DISREGARDING TRAFFIC SIGNALS: 0.1162\n",
      "prim_contributory_cause_FOLLOWING TOO CLOSELY: 0.0966\n",
      "prim_contributory_cause_FAILING TO YIELD RIGHT-OF-WAY: 0.0959\n",
      "crash_hour: 0.0570\n",
      "trafficway_type_FOUR WAY: 0.0478\n",
      "prim_contributory_cause_IMPROPER OVERTAKING/PASSING: 0.0462\n",
      "roadway_surface_cond_UNKNOWN: 0.0343\n",
      "prim_contributory_cause_IMPROPER LANE USAGE: 0.0312\n",
      "prim_contributory_cause_UNABLE TO DETERMINE: 0.0267\n",
      "\n",
      "Example Predictions:\n",
      "\n",
      "Loading models from files...\n",
      "All models successfully loaded!\n",
      "Random Forest: 0.2429\n",
      "Logistic Regression: 0.2198\n",
      "Naive Bayes: 0.0000\n",
      "Neural Network: 0.0001\n",
      "\n",
      "Done! All models trained, evaluated, and saved.\n",
      "Models and results saved in the 'output' directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assume df is loaded  \n",
    "df = pd.read_csv('/kaggle/input/traffic-accidents/traffic_accidents.csv')\n",
    "\n",
    "# For demonstration, you should replace this with your actual dataframe:  \n",
    "from sklearn.datasets import make_classification  \n",
    "\n",
    "\n",
    "# Preprocess data  \n",
    "X, y, categorical_features, numerical_features = preprocess_data(df)  \n",
    "\n",
    "# Train sklearn models  \n",
    "sklearn_models = train_sklearn_models(X, y, categorical_features, numerical_features)  \n",
    "\n",
    "# Save sklearn models  \n",
    "save_models(sklearn_models)  \n",
    "\n",
    "# Evaluate sklearn models  \n",
    "sklearn_results = evaluate_sklearn_models(sklearn_models)  \n",
    "\n",
    "# Train PyTorch neural network  \n",
    "nn_dict = train_neural_network(X, y, categorical_features, numerical_features)  \n",
    "\n",
    "# Evaluate neural network  \n",
    "nn_results = evaluate_neural_network(nn_dict)  \n",
    "\n",
    "# Compare all models  \n",
    "compare_models(sklearn_results, nn_results, sklearn_models['y_test'])  \n",
    "\n",
    "# Analyze feature importance from Random Forest  \n",
    "analyze_feature_importance(sklearn_models['random_forest'],   \n",
    "                          categorical_features,   \n",
    "                          numerical_features)  \n",
    "\n",
    "# Example for prediction  \n",
    "print(\"\\nExample Predictions:\")  \n",
    "# Sample data point  \n",
    "sample = X.iloc[0:1]  \n",
    "\n",
    "# Load models (to demonstrate the loading functionality)  \n",
    "loaded_models = load_models()  \n",
    "\n",
    "# Use loaded models for prediction  \n",
    "if loaded_models:  \n",
    "    rf_prob = predict_with_sklearn_model(loaded_models['random_forest'], sample)  \n",
    "    lr_prob = predict_with_sklearn_model(loaded_models['logistic_regression'], sample)  \n",
    "    nb_prob = predict_with_sklearn_model(loaded_models['naive_bayes'], sample)  \n",
    "    \n",
    "    print(f\"Random Forest: {rf_prob:.4f}\")  \n",
    "    print(f\"Logistic Regression: {lr_prob:.4f}\")  \n",
    "    print(f\"Naive Bayes: {nb_prob:.4f}\")  \n",
    "    \n",
    "    # For neural network, use the dedicated prediction function  \n",
    "    nn_prob = predict_with_pytorch_network(sample)  \n",
    "    if nn_prob is not None:  \n",
    "        print(f\"Neural Network: {nn_prob:.4f}\")  \n",
    "\n",
    "print(\"\\nDone! All models trained, evaluated, and saved.\")  \n",
    "print(f\"Models and results saved in the 'output' directory\")  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6513854,
     "sourceId": 10524793,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 661.792718,
   "end_time": "2025-04-05T17:05:23.823010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T16:54:22.030292",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
